{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e22cedb-c18b-405e-ac51-166ede1a36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sklearn\n",
    "import nltk\n",
    "import re\n",
    "import collections\n",
    "import itertools\n",
    "import wordfreq\n",
    "import symspellpy.symspellpy as symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6fc7712-a22a-4c40-8590-9efc700faf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE WORD EMBEDDING MODULE\n",
    "\n",
    "class Word_Embeddings(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, n_words, max_len = 6400, device = \"cuda\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding_layer = torch.nn.Embedding(n_words, embed_dim, padding_idx = 0)\n",
    "\n",
    "        pe_div_term = torch.exp(-torch.log(torch.tensor(10000.0, device=device)) * torch.arange(0, self.embed_dim, 2, device = device) / self.embed_dim).to(device)\n",
    "        inside_term = torch.arange(0, max_len, device = device).unsqueeze(1) * pe_div_term\n",
    "        positional_encoding = torch.zeros(max_len, self.embed_dim, device = device)\n",
    "        positional_encoding[:, 0::2] = torch.sin(inside_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(inside_term)\n",
    "\n",
    "        self.positional_encoding = positional_encoding\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        device = next(self.embedding_layer.parameters()).device\n",
    "\n",
    "        seq_lens = [len(sentence) for sentence in inputs]\n",
    "        max_len = max(seq_lens)\n",
    "        tokens = torch.zeros((len(inputs), max_len), dtype = torch.long, device = device)\n",
    "        for i, sentence in enumerate(inputs):\n",
    "            tokens[i, 0:len(sentence)] = sentence.to(device)\n",
    "\n",
    "        embeddings = self.embedding_layer(tokens)\n",
    "\n",
    "        attn_mask = torch.zeros(len(seq_lens), max_len, dtype = torch.bool, device = device)\n",
    "\n",
    "        for i in range(len(seq_lens)):\n",
    "            embeddings[i, 0:seq_lens[i], :] += self.positional_encoding[0:seq_lens[i], :]\n",
    "            attn_mask[i, seq_lens[i]:] = 1\n",
    "\n",
    "        return embeddings, attn_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef678adb-2af9-4e5c-8f2a-c7006650e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE ATTENTION MODULE\n",
    "\n",
    "class Multi_Head_Attention_Layer(torch.nn.Module):\n",
    "    def __init__(self, in_len, out_len, n_heads, hidden_len, cls_only = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_len = in_len\n",
    "        self.out_len = out_len\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_len = hidden_len\n",
    "        self.cls_only = cls_only\n",
    "\n",
    "        self.key_proj = torch.nn.Linear(in_len, n_heads * hidden_len)\n",
    "        self.query_proj = torch.nn.Linear(in_len, n_heads * hidden_len)\n",
    "        self.value_proj = torch.nn.Linear(in_len, n_heads * out_len)\n",
    "\n",
    "        self.linear_transform = torch.nn.Linear(n_heads * out_len, in_len)\n",
    "        self.layer_norm = torch.nn.LayerNorm(in_len)\n",
    "    \n",
    "    def forward(self, inputs, attention_mask = None):\n",
    "        batch_size, seq_len = inputs.shape[0:2]\n",
    "        keys = self.key_proj(inputs).view(batch_size, seq_len, self.n_heads, self.hidden_len).transpose(1, 2)\n",
    "        queries = self.query_proj(inputs).view(batch_size, seq_len, self.n_heads, self.hidden_len).transpose(1, 2)\n",
    "        values = self.value_proj(inputs).view(batch_size, seq_len, self.n_heads, self.out_len).transpose(1, 2)\n",
    "\n",
    "        if self.cls_only:\n",
    "            queries = queries[:, :, 0:1, :]\n",
    "\n",
    "        relations = torch.matmul(queries, keys.transpose(-1, -2)) / self.hidden_len**0.5\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            relations = relations.masked_fill(attention_mask, float('-inf'))\n",
    "        \n",
    "        relations = torch.softmax(relations, dim = -1)\n",
    "        attention = torch.matmul(relations, values)\n",
    "        attention = attention.permute(0, 2, 1, 3)\n",
    "        \n",
    "        if self.cls_only:\n",
    "            attention = attention.reshape(attention.shape[0], self.n_heads * self.out_len)\n",
    "        else:\n",
    "            attention = attention.reshape(attention.shape[0], attention.shape[1], self.n_heads * self.out_len)\n",
    "\n",
    "        output = self.linear_transform(attention)\n",
    "        output = self.layer_norm(output)\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c883d8ea-60f5-4375-9785-12582650eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE TRANSFORMER MODULE\n",
    "\n",
    "class Transformer_Classifier(torch.nn.Module):\n",
    "    def __init__(self, n_classes, embedding_dim, n_heads, attention_dim, key_query_dim, fc1_dim, fc2_dim, n_words, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Embedding_model = Word_Embeddings(embedding_dim, n_words)\n",
    "\n",
    "        self.Layer1 = Multi_Head_Attention_Layer(in_len=embedding_dim, out_len=attention_dim, n_heads=n_heads, hidden_len=key_query_dim)\n",
    "\n",
    "        self.Layer2 = Multi_Head_Attention_Layer(in_len=embedding_dim, out_len=attention_dim, n_heads=n_heads, hidden_len=key_query_dim, cls_only = True)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = torch.nn.Sequential(torch.nn.Linear(embedding_dim, fc1_dim),\n",
    "                                      torch.nn.ReLU(),\n",
    "                                      torch.nn.Linear(fc1_dim, fc2_dim),\n",
    "                                      torch.nn.ReLU(),\n",
    "                                      torch.nn.Linear(fc2_dim, n_classes))\n",
    "        \n",
    "        self.cls_token = torch.nn.Parameter((2 * torch.rand(1, 1, embedding_dim) - 1))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        single = False\n",
    "        if not isinstance(inputs, list):\n",
    "            single = True\n",
    "            inputs = [inputs]\n",
    "        embeddings, attention_masks = self.Embedding_model(inputs)\n",
    "        embeddings = torch.cat((self.cls_token.expand(embeddings.shape[0], -1, -1), embeddings), dim = 1)\n",
    "        attention_masks = torch.cat((torch.zeros(attention_masks.shape[0], 1, dtype = torch.bool, device = attention_masks.device), attention_masks), dim = 1)\n",
    "\n",
    "        outputs = embeddings + self.Layer1(embeddings, attention_masks)\n",
    "        outputs = self.Layer2(outputs, attention_masks)\n",
    "        outputs = self.fc(self.dropout(outputs))\n",
    "\n",
    "        if single:\n",
    "            outputs = outputs[0, :, :]\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbb24aac-bdcc-4d7c-be6e-facb8bc489c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE DATASET CLASS AND REQUIRED METHODS\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word2idx, n_classes, max_len = 1024):\n",
    "        self.word2idx = word2idx\n",
    "        self.texts = [list(map((lambda w : self.word2idx.get(w, 0)), sentence.split())) for sentence in texts]\n",
    "        self.labels = labels.apply(lambda x : self.one_hot_encode(x, n_classes).unsqueeze(0))\n",
    "        self.max_len = max_len\n",
    "        #print(pd.Series([len(s) for s in self.texts]).describe())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.texts[idx][0:self.max_len], dtype=torch.long), self.labels.iloc[idx])\n",
    "    \n",
    "    @classmethod\n",
    "    def create_word_idx_map(cls, texts, freq_threshold = 10):\n",
    "        words = texts.str.findall(r\"\\w+\")\n",
    "        word2idx = {\"<PAD>\" : 0}\n",
    "        curr_idx = 1\n",
    "        words = list(itertools.chain(*words))\n",
    "        unique_words = collections.Counter(words)\n",
    "\n",
    "        for word, freq in unique_words.items():\n",
    "            if word not in word2idx and freq >= freq_threshold:\n",
    "                word2idx[word] = curr_idx\n",
    "                curr_idx += 1\n",
    "        \n",
    "        return word2idx\n",
    "\n",
    "    @classmethod\n",
    "    def collate_fn(cls, batch):\n",
    "        texts, labels = zip(*batch)\n",
    "        texts = list(texts)\n",
    "        labels = torch.cat(labels, dim = 0)\n",
    "        return texts, labels\n",
    "    \n",
    "    @classmethod\n",
    "    def one_hot_encode(cls, label, n_classes):\n",
    "        vector = torch.zeros(n_classes)\n",
    "        vector[label] = 1\n",
    "        return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2014ac93-7618-4184-998a-bb074c66706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE TRAINING FUNCTIONS\n",
    "\n",
    "def train_one_epoch(model, loader, loss_fn, optimizer, device = \"cpu\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    prev_loss = 0\n",
    "    n_batch = 0\n",
    "    total_batches = len(loader)\n",
    "    print_batches = 100\n",
    "\n",
    "    for texts, labels in loader:\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(texts)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_batch += 1\n",
    "\n",
    "        if n_batch % print_batches == 0:\n",
    "            print(f\"Batch {n_batch} / {total_batches} : Train loss = {(total_loss - prev_loss) / print_batches}\")\n",
    "            prev_loss = total_loss\n",
    "            \n",
    "    return total_loss / total_batches\n",
    "\n",
    "def evaluate(model, loader, loss_fn = torch.nn.CrossEntropyLoss(), device = \"cpu\"):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in loader:\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            total_loss += loss_fn(outputs, labels)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            labels = torch.argmax(labels, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "    \n",
    "    val_loss = total_loss/len(loader)\n",
    "    accuracy = (pd.Series(all_preds) == pd.Series(all_labels)).sum() / len(all_labels)\n",
    "    return (val_loss, accuracy, sklearn.metrics.classification_report(all_labels, all_preds, zero_division = 0))\n",
    "\n",
    "def run_training(model, train_loader, test_loader, epochs = 10, lr = 1e-3, weight_decay = 1e-5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 2, factor = 0.2)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f\"\\nEpoch {epoch}\\n\")\n",
    "        train_loss = train_one_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "        report = evaluate(model, test_loader, loss_fn, device)\n",
    "        scheduler.step(metrics = report[0])\n",
    "        print(report[2])\n",
    "        print(f\"\\n\\nTrain Loss: {train_loss}\")\n",
    "        print(f\"Test Loss: {report[0]}\")\n",
    "        print(f\"Test accuracy: {report[1]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b1f9c-7e7c-4728-81e7-f7e13f32b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING DATA\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df.dropna(subset=['Text', 'Category'], inplace=True)\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words.discard(\"not\")\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "common_words = wordfreq.top_n_list('en', 400000)\n",
    "\n",
    "sym_spell = symspellpy.SymSpell(max_dictionary_edit_distance=3)\n",
    "for word in common_words:\n",
    "    sym_spell.create_dictionary_entry(word, int(wordfreq.word_frequency(word, \"en\") * 100000))\n",
    "\n",
    "\n",
    "def correct_word(word):\n",
    "    suggestions = sym_spell.lookup(word, symspellpy.Verbosity.CLOSEST, max_edit_distance=3)\n",
    "    if suggestions:\n",
    "        return suggestions[0].term\n",
    "    else:\n",
    "        return word.replace('.', 'e')\n",
    "\n",
    "\n",
    "def get_pos_tag(tag):\n",
    "    if tag[0] == 'V':\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif tag[0] == 'J':\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif tag[0] == 'R':\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "\n",
    "def match_word(patterned_word):\n",
    "    regex = re.compile('^' + patterned_word + '$')\n",
    "    for word in common_words:\n",
    "        if len(word) == len(patterned_word) and regex.match(word):\n",
    "            return word\n",
    "    else:\n",
    "        return patterned_word.replace('.', 'e')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\b\\S*(https|www|linkedin)\\S*\\b', '', text).strip()\n",
    "    text = re.sub(r\"[^a-z\\s]\", '', text)\n",
    "    text = text.replace('claire', '.')\n",
    "    words = [correct_word(word) for word in text.split()]\n",
    "    words = nltk.pos_tag(words)\n",
    "    words = [lemmatizer.lemmatize(w[0], pos = get_pos_tag(w[1])) for w in words if w[0] not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df['Text'] = df['Text'].apply(preprocess)\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(df['Category'].unique())}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "df['label'] = df['Category'].map(label2id)\n",
    "\n",
    "print(\"Preprocessing done\")\n",
    "\n",
    "df = df[['label', 'Text']].reset_index(drop=True)\n",
    "df.to_csv(\"Processed_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "950ffd9b-ff25-49cb-87c4-b8a3c99252ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD DATA AND MODEL\n",
    "\n",
    "id2label = {0: 'Accountant', 1: 'Advocate', 2: 'Agriculture', 3: 'Apparel', 4: 'Architecture', 5: 'Arts', 6: 'Automobile', 7: 'Aviation', 8: 'Banking', 9: 'Blockchain', 10: 'BPO', 11: 'Building and Construction', 12: 'Business Analyst', 13: 'Civil Engineer', 14: 'Consultant', 15: 'Data Science', 16: 'Database', 17: 'Designing', 18: 'DevOps', 19: 'Digital Media', 20: 'DotNet Developer', 21: 'Education', 22: 'Electrical Engineering', 23: 'ETL Developer', 24: 'Finance', 25: 'Food and Beverages', 26: 'Health and Fitness', 27: 'Human Resources', 28: 'Information Technology', 29: 'Java Developer', 30: 'Management', 31: 'Mechanical Engineer', 32: 'Network Security Engineer', 33: 'Operations Manager', 34: 'PMO', 35: 'Public Relations', 36: 'Python Developer', 37: 'React Developer', 38: 'Sales', 39: 'SAP Developer', 40: 'SQL Developer', 41: 'Testing', 42: 'Web Designing'}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "df = pd.read_csv(\"Processed_train.csv\")\n",
    "\n",
    "word2idx = TextDataset.create_word_idx_map(df['Text'], freq_threshold = 3)\n",
    "train_texts, val_texts, train_labels, val_labels = sklearn.model_selection.train_test_split(df['Text'], df['label'], test_size=0.2, random_state=100)\n",
    "\n",
    "train_dataset = TextDataset(train_texts.reset_index(drop=True), train_labels.reset_index(drop=True), word2idx, n_classes = len(id2label), max_len=1024)\n",
    "test_dataset = TextDataset(val_texts.reset_index(drop=True), val_labels.reset_index(drop=True), word2idx, n_classes = len(id2label), max_len=1024)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 16, shuffle = True, collate_fn = TextDataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 16, shuffle = False, collate_fn = TextDataset.collate_fn)\n",
    "\n",
    "model= Transformer_Classifier(n_classes=len(id2label), embedding_dim=256, n_heads=5, attention_dim=128, key_query_dim=128, fc1_dim=128, fc2_dim=64, n_words=len(word2idx), dropout=0.3)\n",
    "model.load_state_dict(torch.load(\"Transformer_weights4.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c54531-2bde-427c-8c29-35265a666b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN TRAINING\n",
    "\n",
    "run_training(model, train_dataloader, test_dataloader, 10, lr=4e-6, weight_decay = 2e-2)\n",
    "torch.save(model.state_dict(), \"Transformer_weights4.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5000e2a-0038-4f0c-8445-df217ab4d34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.80      0.69        65\n",
      "           1       0.71      0.89      0.79        47\n",
      "           2       0.36      0.23      0.28        64\n",
      "           3       0.22      0.21      0.21        48\n",
      "           4       0.51      0.36      0.42        77\n",
      "           5       0.64      0.61      0.62        69\n",
      "           6       0.34      0.31      0.32        65\n",
      "           7       0.85      0.72      0.78        64\n",
      "           8       0.37      0.34      0.35        53\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       0.81      0.36      0.50        47\n",
      "          11       0.66      0.85      0.74        73\n",
      "          12       0.63      0.60      0.61        77\n",
      "          13       0.71      0.78      0.75        64\n",
      "          14       0.52      0.48      0.50        64\n",
      "          15       0.76      0.68      0.72        65\n",
      "          16       0.67      0.60      0.63        50\n",
      "          17       0.48      0.40      0.44        60\n",
      "          18       0.87      0.98      0.92        54\n",
      "          19       0.78      0.84      0.81        74\n",
      "          20       0.40      0.32      0.35        63\n",
      "          21       0.70      0.68      0.69        95\n",
      "          22       0.84      0.91      0.87        89\n",
      "          23       0.66      0.61      0.64        77\n",
      "          24       0.48      0.45      0.47        77\n",
      "          25       0.89      0.86      0.88        29\n",
      "          26       0.88      0.94      0.91        62\n",
      "          27       0.85      0.91      0.88        76\n",
      "          28       0.54      0.50      0.52        56\n",
      "          29       0.33      0.66      0.44        65\n",
      "          30       0.35      0.22      0.27        69\n",
      "          31       0.87      0.94      0.90        82\n",
      "          32       0.79      0.94      0.86        53\n",
      "          33       0.61      0.79      0.69        70\n",
      "          34       0.48      0.53      0.51        58\n",
      "          35       0.74      0.83      0.78        59\n",
      "          36       0.34      0.23      0.27        44\n",
      "          37       0.00      0.00      0.00        44\n",
      "          38       0.58      0.76      0.65        74\n",
      "          39       0.45      0.39      0.42        51\n",
      "          40       0.56      0.67      0.61        67\n",
      "          41       0.68      0.83      0.75        69\n",
      "          42       0.68      0.63      0.66        65\n",
      "\n",
      "    accuracy                           0.62      2678\n",
      "   macro avg       0.59      0.60      0.58      2678\n",
      "weighted avg       0.61      0.62      0.61      2678\n",
      "\n",
      "Test Loss: 1.3355854749679565\n",
      "Test accuracy: 0.623226288274832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#EVALUATION\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "report = evaluate(model, test_dataloader, device = device)\n",
    "print(report[2])\n",
    "print(f\"Test Loss: {report[0]}\")\n",
    "print(f\"Test accuracy: {report[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b79a3f-4e77-4852-a9b9-d7d5fd2ac177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
