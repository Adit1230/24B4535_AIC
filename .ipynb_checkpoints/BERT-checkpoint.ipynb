{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c39076-079c-4350-8db5-3bac763dd05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT LIBRARIES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "import transformers\n",
    "import wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb2013-aee7-4f84-bd0c-20c9a25fe333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df.dropna(subset=['Text', 'Category'], inplace=True)\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words.discard(\"not\")\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "common_words = wordfreq.top_n_list('en', 400000)\n",
    "\n",
    "sym_spell = symspellpy.SymSpell(max_dictionary_edit_distance=3)\n",
    "for word in common_words:\n",
    "    sym_spell.create_dictionary_entry(word, int(wordfreq.word_frequency(word, \"en\") * 100000))\n",
    "\n",
    "\n",
    "def correct_word(word):\n",
    "    suggestions = sym_spell.lookup(word, symspellpy.Verbosity.CLOSEST, max_edit_distance=3)\n",
    "    if suggestions:\n",
    "        return suggestions[0].term\n",
    "    else:\n",
    "        return word.replace('.', 'e')\n",
    "\n",
    "\n",
    "def get_pos_tag(tag):\n",
    "    if tag[0] == 'V':\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif tag[0] == 'J':\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif tag[0] == 'R':\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "\n",
    "def match_word(patterned_word):\n",
    "    regex = re.compile('^' + patterned_word + '$')\n",
    "    for word in common_words:\n",
    "        if len(word) == len(patterned_word) and regex.match(word):\n",
    "            return word\n",
    "    else:\n",
    "        return patterned_word.replace('.', 'e')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\b\\S*(https|www|linkedin)\\S*\\b', '', text).strip()\n",
    "    text = re.sub(r\"[^a-z\\s]\", '', text)\n",
    "    text = text.replace('claire', '.')\n",
    "    words = [correct_word(word) for word in text.split()]\n",
    "    words = nltk.pos_tag(words)\n",
    "    words = [lemmatizer.lemmatize(w[0], pos = get_pos_tag(w[1])) for w in words if w[0] not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df['Text'] = df['Text'].apply(preprocess)\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(df['Category'].unique())}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "df['label'] = df['Category'].map(label2id)\n",
    "\n",
    "print(\"Preprocessing done\")\n",
    "\n",
    "df = df[['label', 'Text']].reset_index(drop=True)\n",
    "df.to_csv(\"Processed_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f6533dc-f855-49cc-b292-ff36c8a0b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD DATA\n",
    "\n",
    "id2label = {0: 'Accountant', 1: 'Advocate', 2: 'Agriculture', 3: 'Apparel', 4: 'Architecture', 5: 'Arts', 6: 'Automobile', 7: 'Aviation', 8: 'Banking', 9: 'Blockchain', 10: 'BPO', 11: 'Building and Construction', 12: 'Business Analyst', 13: 'Civil Engineer', 14: 'Consultant', 15: 'Data Science', 16: 'Database', 17: 'Designing', 18: 'DevOps', 19: 'Digital Media', 20: 'DotNet Developer', 21: 'Education', 22: 'Electrical Engineering', 23: 'ETL Developer', 24: 'Finance', 25: 'Food and Beverages', 26: 'Health and Fitness', 27: 'Human Resources', 28: 'Information Technology', 29: 'Java Developer', 30: 'Management', 31: 'Mechanical Engineer', 32: 'Network Security Engineer', 33: 'Operations Manager', 34: 'PMO', 35: 'Public Relations', 36: 'Python Developer', 37: 'React Developer', 38: 'Sales', 39: 'SAP Developer', 40: 'SQL Developer', 41: 'Testing', 42: 'Web Designing'}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "df = pd.read_csv(\"Processed_train.csv\")\n",
    "\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['Text'].to_list(), df['label'].to_list(), test_size=0.2, random_state=100\n",
    ")\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
    "            'labels': torch.tensor(self.labels[idx])\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, train_labels)\n",
    "val_dataset = TextDataset(val_encodings, val_labels)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89611936-784b-41fb-8823-3167193f4e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=43, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD MODEL\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', num_labels=len(id2label)\n",
    ")\n",
    "model.load_state_dict(torch.load(\"bert_weights2.pt\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d8ff1-d1c0-4bc4-93db-acb1df8dbd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN MODEL\n",
    "\n",
    "learning_rate = 1e-6\n",
    "epochs = 2\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay = 0.07)\n",
    "num_training_steps = epochs * len(train_loader)\n",
    "lr_scheduler = transformers.get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\\n\\n\")\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    prev_loss = 0\n",
    "    batch_no = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_no += 1\n",
    "        if batch_no % 100 == 0:\n",
    "            print(f\"Batch {batch_no} / {len(train_loader)} : Train loss = {(total_loss - prev_loss)/100}\")\n",
    "            prev_loss = total_loss\n",
    "\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    preds, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_val_loss += loss.item()\n",
    "            preds.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
    "            true_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ca4f1-d513-42b9-b9f0-52a3b1923a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE MODEL\n",
    "\n",
    "torch.save(model.state_dict(), 'bert_weights2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989e1d00-3f3a-4443-823e-c39ba5c35eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE MODEL\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, preds, target_names=list(label2id.keys())))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training/Validation Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "599ccb06-200f-418e-8fe6-53bdb5061afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score   support\n",
      "\n",
      "               Accountant       0.81      0.95      0.87        65\n",
      "                 Advocate       0.84      0.89      0.87        47\n",
      "              Agriculture       0.80      0.73      0.76        64\n",
      "                  Apparel       0.64      0.73      0.68        48\n",
      "             Architecture       0.74      0.66      0.70        77\n",
      "                     Arts       0.81      0.88      0.85        69\n",
      "               Automobile       0.57      0.46      0.51        65\n",
      "                 Aviation       0.92      0.92      0.92        64\n",
      "                  Banking       0.81      0.81      0.81        53\n",
      "               Blockchain       1.00      0.75      0.86         4\n",
      "                      BPO       0.92      0.70      0.80        47\n",
      "Building and Construction       0.72      0.78      0.75        73\n",
      "         Business Analyst       0.87      0.92      0.89        77\n",
      "           Civil Engineer       0.98      0.91      0.94        64\n",
      "               Consultant       0.74      0.67      0.70        64\n",
      "             Data Science       0.93      0.82      0.87        65\n",
      "                 Database       0.76      0.76      0.76        50\n",
      "                Designing       0.94      0.78      0.85        60\n",
      "                   DevOps       0.90      0.96      0.93        54\n",
      "            Digital Media       0.85      0.89      0.87        74\n",
      "         DotNet Developer       0.79      0.76      0.77        63\n",
      "                Education       0.88      0.82      0.85        95\n",
      "   Electrical Engineering       0.90      0.92      0.91        89\n",
      "            ETL Developer       0.76      0.74      0.75        77\n",
      "                  Finance       0.84      0.81      0.82        77\n",
      "       Food and Beverages       0.87      0.93      0.90        29\n",
      "       Health and Fitness       0.72      0.87      0.79        62\n",
      "          Human Resources       0.99      0.95      0.97        76\n",
      "   Information Technology       0.82      0.73      0.77        56\n",
      "           Java Developer       0.74      0.95      0.83        65\n",
      "               Management       0.43      0.51      0.46        69\n",
      "      Mechanical Engineer       0.95      0.96      0.96        82\n",
      "Network Security Engineer       0.86      0.94      0.90        53\n",
      "       Operations Manager       0.81      0.93      0.87        70\n",
      "                      PMO       0.59      0.55      0.57        58\n",
      "         Public Relations       0.87      0.90      0.88        59\n",
      "         Python Developer       0.42      0.61      0.50        44\n",
      "          React Developer       0.88      0.32      0.47        44\n",
      "                    Sales       0.78      0.66      0.72        74\n",
      "            SAP Developer       0.91      0.96      0.93        51\n",
      "            SQL Developer       0.67      0.60      0.63        67\n",
      "                  Testing       0.84      0.90      0.87        69\n",
      "            Web Designing       0.82      0.77      0.79        65\n",
      "\n",
      "                 accuracy                           0.80      2678\n",
      "                macro avg       0.81      0.79      0.79      2678\n",
      "             weighted avg       0.80      0.80      0.80      2678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TEST MODEL\n",
    "\n",
    "model.eval()\n",
    "total_val_loss = 0\n",
    "preds, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        total_val_loss += loss.item()\n",
    "        preds.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
    "        true_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "print(classification_report(true_labels, preds, target_names=list(label2id.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d259e-5a47-4837-9a57-19d13128e82f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
