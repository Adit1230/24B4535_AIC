Setup:
Run pip install -r requirements.txt to installed required libraries. Then install pytorch using instructions given in https://pytorch.org/get-started/locally/
For using the rag chatbot, create an api key on the groq cloud platform and set the environment variable GROQ_API_KEY to the api key. Also replace the file path with a path of the pdf file of your choice

Bert:
First, I downloaded the data and went through some examples. I noticed that there were multiple links to websites. I then cleaned it by converting everything into lower case, removing numbers and special characters and removing all links. I then lemmatized it and removed stop words except the word "not". This is because this word might provide some useful data. Later,looking at unique words in the text, I found that the number of unique words was very large. So, I looked at the texts again and realised that in some words, letters were randomly replaced with the substring "claire". So, I replaced every instance of the word claire with a '.' . Then I proceeded to spell check all words by comparing them with a large list of english words and replacing it with the closest and most frequently used word, effectively correcting any spelling errors and also replacing the '.' with the actual letters. I also converted the labels to numerical ids. I then saved the cleaned data to another file called Processed_train.csv . 
Then I split the data into train and test datasets. I used the pretrained model "distilbert-base-uncased". I used the tokenizer of this model to tokenise the words in the text and then loaded it into a dataset. Then I loaded the pretrained model and set it to classify the text into 43 classes. I trained the model for 5 epochs using a learning rate of 1e-5 with some warmup steps and saved the model weights. Then I trained it again for 2 epochs with a lower learning rate and higher weight decay. I was able to achieve an accuracy of over 80%

From scratch attention:
First I made a class for word embeddings. It takes in a list (batch) of tensors of tokens and creates word embedding vectors for it. It first combines all the tensors of different lengths into one rank 2 padded tensor. Then it uses torch.nn.embedding to create word embedding. Then it adds sinusoidal positional encoding to it. It also computes an attention mask which identifies the padded tokens. Then I created a class for multiheaded attention layer. It takes in a sequence of word embeddings (vectors) and computes key, query and value vectors for each token using linear transformations. Then each query vector is multiplied with each query vector to create a relations matrix which represents how related any two tokens are to each other. The for each token, we get a transformed vector representation which represents the token in context of the other words by calculating the sum of the value vectors for each word weighted by the relation matrix values. There are multiple such heads. The outputs of all the heads are concatenated and linearly transformed to an output of desired size. Then layer normalisation is applied. Then I created the transformer classifier class. It contains a word embedding layer and 2 attention layers followed by a fully connected layer to convert the output into the final classification output. There is a residual connection between the word embedding layer and the output of the first attention layer. It also contains a learnable cls token which is injected to the start of the sequence of word embeddings. Then, the representation of the cls token after the two attention alyers is used for classification discarding the other representations. This allows the model to classify text irrespective of the number of tokens. I implemented a feature in the attention model to compute the representation of only the first vector (the cls token representation)  based on initialisation in order to save computational costs. I also included a dropout layer to improve training. I then trained it using Adam optimiser and reduce LR or plateau LR optimiser. I first experimented with step LR but that didnt work that well so I switched to reduce LR on plateau. I then trained the model for multiple epochs gradually reducing learning rate, increasing weight decay and changing dropout rate. I was able to achieve accuracy of over 60% and could possibly achieve more with more training.
Training a transformer from scratch requires more data and more number of training epochs, but it may be better if we have such data due to it being fine tuned for that specific task only. Using a pretrained model like BERT allows us to leverage transfer learning so that we can achive good accuracy without that much training.

Fashion classifier:
First, I loaded the Fashion MNIST dataset from two the training and testing files. Each image in this dataset is 28x28 pixels, but the images were flattened into a single row of numbers in the CSV. I reshaped the data back to 28x28 images and separated the labels from the pixel values. Then, I created a custom dataset class called NumpyDataset. This class takes the 28x28 grayscale images and upscales them to 224x224 and converts the image into 3 channels by repeating the pixels so that they can be used with ResNet50, which expects larger RGB images. The pixel values were normalized to be between 0 and 1. This dataset class returns a tensor image and its label. I created data loaders for both the training and testing sets, using a batch size of 64.
Then I used a pretrained ResNet50 model from torchvision. This model was trained on the ImageNet dataset. I replaced the final fully connected layer with a new one that first applies dropout and then outputs predictions for 10 classes (since Fashion MNIST has 10 clothing categories). I then trained only the final fully connected layer, freezing the rest of the model. This uses the meaningful features extracted by the pretrained model and uses it to classify the images. Then, for finetuning I unfroze the second last layer and trained it in order to improve the accuracy firther by training more of the model for this specific task. The training loop runs for multiple epochs. For each batch, it computes the predictions, calculates the cross-entropy loss, backpropagates the error, and updates the model's parameters using the Adam optimizer. I was able to achieve over 90% accuracy with this method

RAG chatbot:
I created an account on groq cloud platform and created an api key. Then I created a class called RAGChatbot. This class takes the file path of the pdf to be scanned and the groq api key as paremeters for initialisation. It then reads the pdf, divides the text into chunks with some overlap and then builds a FAISS index for it and stores it. Methods are written in the class for each of these tasks. Then a groq client is created and stored. A system prompt is there to direct the llm to answer the query based on the context provided. An empty message history is initialised. The message history is a list of dictionaries which contain the role (assistant/user) and the content of the message. When a question is asked to the chatbot, first the FAISS index is searched using the query and relevant context chunks are retrieved. Then, a prompt is contructed by concatenating the system prompt, context and the query. This prompt is sent to the chatbot along with the message history by concatenating the prompt as a message with the history and passing it to the client. Then the answer is retrieved. The question (not prompt) and answer are added to the message history and the answer is finally returned. Outside of the class the program to be executed is written which first initialises the chatbot and then runs a conversation loop. The user is able to chat with the chatbot until they type exit or quit at which point the program terminates.

Note:
I didn't know we were supposed to use jupyter notebook files and so, I wrote .py files only. Then I changed it which is why not many outputs are visible. I was also having some connectivity issues and git cache issues related to large files so the commits were late. Could not upload bert weights because github doesnt accept large files.